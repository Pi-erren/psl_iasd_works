{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IErereKOQzO1"
      },
      "source": [
        "# Frozen Lake\n",
        "The Frozen Lake environment is an uncertain grid world in which you start from an initial state (the top leftmost square) and move to a final state (the bottom rightmost square). The environment is uncertain because you are walking on a frozen lake and the ice thickness varies. So you can fall into the water in some squares. Also, the ice is more slippery in some places, so taking a step can take you further than expected... and if the wind gets in the way...\n",
        "Instead of trying to estimate the transition pattern, we'll use SARSA and Q-learning to solve this problem.\n",
        "Use the Frozen Lake environment to implement SARSA and Q-learning. First use the environment with a 4x4 grid to test your algorithms, then you should be able to use them for the 16x16 grid."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xa9Prd9RAYq"
      },
      "source": [
        "#FrozenLake - familiarization with the environment\n",
        "Evaluate a random policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmZN3dbpQtyQ",
        "outputId": "f32202f7-0b6a-464b-c198-c74f2da2b0f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environnement with  16  states and  4  actions\n",
            "state number: 4\n",
            "state number: 8\n",
            "state number: 8\n",
            "state number: 8\n",
            "state number: 4\n",
            "state number: 5\n",
            "Episod ended after 6 iterations\n",
            "Reward obtained: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gymnasium as gym\n",
        "\n",
        "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "numStates = env.observation_space.n\n",
        "numActions = env.action_space.n\n",
        "print(\"Environnement with \", numStates, \" states and \", numActions, \" actions\")\n",
        "#\n",
        "# env.reset() resets and starts a new episode\n",
        "#\n",
        "state = env.reset()\n",
        "nbIt=0\n",
        "rew=[]\n",
        "done=False\n",
        "while not done:\n",
        "  #\n",
        "  # env.step(action) executes an action in the current state\n",
        "  # the method returns\n",
        "  #    • the next state\n",
        "  #    • the immediate reward\n",
        "  #    • a boolean that tells whether the episode has ended\n",
        "  #    • another argument that is usefull for debugging purposes\n",
        "  #\n",
        "  nextState, reward, done, trunc, info = env.step(np.random.randint(4))\n",
        "  print(\"state number:\",nextState)\n",
        "\n",
        "  nbIt+=1\n",
        "  rew = rew+[reward]\n",
        "print(\"Episod ended after {} iterations\".format(nbIt))\n",
        "print(\"Reward obtained:\",rew)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5rCWVfFik-q",
        "outputId": "894cecba-d6d9-4c6f-a073-44959383d3ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, {'prob': 1})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.reset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GweIa9uizLx",
        "outputId": "65163daa-ad19-45a2-acd7-05a16d772d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0.0, False, False, {'prob': 0.3333333333333333})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.step(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-OAXC5wSj1H"
      },
      "source": [
        "## $\\epsilon$-greedy\n",
        "\n",
        "Implement a function that returns an action with the $\\epsilon$-greedy strategy:\n",
        "* explores with probability $1-\\epsilon$: here we choose the action with the best value of $q[s]$\n",
        "* explores with a probability $\\epsilon$: we choose an action in a uniform way on all the actions.\n",
        "\n",
        "You can choose different signatures for the function:\n",
        "either by passing it:\n",
        " * the parameter $\\epsilon$\n",
        " * the Q table\n",
        " * the s state in which the action will be executed\n",
        " * so the call will have the form `action = epsGreedy(eps, Q, s)`\n",
        "\n",
        " Alternatively, you can give only the value of $\\epsilon$ and vector Q(s) (whose dimension is the number of actions). The call will then have the form `action = epsGreedy(eps, q)`\n",
        "\n",
        "*Please note* One can imagine the particular case where there are several occurrences of the max value in the vector `Q(s)`. In this case, one should not *always* choose the same action, but rather choose one of the ex-aequo actions at random.\n",
        "This case may not be so exotic, especially at the beginning of the learning process, when all values are zero. To explore, it is then desirable to repeat the same choice!\n",
        "\n",
        "\n",
        "For those unfamiliar with python, take a look at the small code example below to illustrate some of the functions in the `numpy` library\n",
        "- The function `np.random.rand()` draws a value uniformly between 0 and 1.\n",
        "- The function `np.random.choice` draws a value uniformly from a set.\n",
        "- The function `np.argwhere(l)` allows to give the indices where the input of the vector l is non-zero. We can therefore couple a call to `np.argwhere` with a test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ3KCEEbRGtY",
        "outputId": "054d734d-2042-4941-d7dd-8a8a031e428e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.711624653012464\n",
            "result from the 10 draws: [3. 1. 5. 1. 3. 3. 3. 5. 5. 3.]\n",
            "the proportion of each value over these 10 draws are  [0.2 0.5 0.3]\n",
            "the indices of the list val in which the value is 3 are: [[0]\n",
            " [4]\n",
            " [5]\n",
            " [6]\n",
            " [9]]\n"
          ]
        }
      ],
      "source": [
        "val = np.random.rand()\n",
        "print(val)\n",
        "val=np.zeros(10)\n",
        "# we draw a sample uniformly in the set {1, 3, 5}\n",
        "for i in range(10):\n",
        "  val[i]=np.random.choice([1,3,5])\n",
        "_, count = np.unique(val,return_counts=True)\n",
        "print(\"result from the 10 draws:\", val)\n",
        "print(\"the proportion of each value over these 10 draws are \", count/10)\n",
        "indices3 = np.argwhere(val==3)\n",
        "print(\"the indices of the list val in which the value is 3 are:\", indices3)\n",
        "\n",
        "#def epsGreedy():\n",
        "   à compléter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SORLRPMQTbVZ"
      },
      "source": [
        "## Testing a policy\n",
        "\n",
        "When learning, it is necessary to explore, so when analyzing the performance during learning, one must keep in mind that some of the choices are made at random. After learning, one can test by being gluttonous: in each state, one always chooses the action that gives the highest value of `Q`.\n",
        "\n",
        "Implement a method that takes as parameter a fixed number of episodes, a `Q` table, and executes the gluttonous policy. The method returns the average value of the sum of the rewards over the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi4UY3bPTSvE"
      },
      "outputs": [],
      "source": [
        "# implement!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh1om8TMTxnc"
      },
      "source": [
        "### SARSA\n",
        "\n",
        "Implement a SARSA function that takes as parameter\n",
        " * a number of episodes used for learning\n",
        " * $\\gamma$ the discount rate\n",
        " * $\\alpha$ the learning rate (which is found when updating the values of Q)\n",
        " * $\\epsilon$ the parameter for the $\\epsilon$-greedy method.\n",
        "\n",
        "Your function must at least return the table $Q: S \\times A$. You will find below a function $plotQ$ which generates a representation of the table $Q$: for each cell will be drawn the best action according to $Q$ and the color will represent the value of this action.\n",
        "\n",
        "To visualize the progress made during learning, your SARSA function can also return a sequence of values. For example,\n",
        " * the sequence of rewards (total or average) obtained on each learning episode\n",
        " * the value of the best action for the starting state at the end of each episode.\n",
        " * Instead of using the values obtained during the training, you can also periodically evaluate the current policy (without exploration). To do this, you can calculate the performance over a small number of episodes and return the average. This method has the advantage of evaluating the policy without exploration (thus a better evaluation of the policy), but can be expensive in computation time depending on the frequency of execution and the number of episodes used for the evaluation.\n",
        "\n",
        "When generating the graph, you should visualize if the algorithm has managed to improve the performance. You can either plot the value of each episode directly. To get a smoother curve, you can also calculate an average over a window of $k$ episodes (the $runningAvg$ function does this job).\n",
        "\n",
        "Note that Frozen lake is considered solved when\n",
        " * it reaches the goal in 78% of the episodes for the 4x4 grid.\n",
        " * a priori, we can reach 100% for the 8x8 grid\n",
        "\n",
        "Some ideas to help with the debug:\n",
        " * you can also look at whether most of the state-action pairs have been executed.\n",
        " * You can choose as parameters (the code I wrote worked with these parameters, obviously, you can try with others later).\n",
        "   * $\\epsilon=0.2$\n",
        "   * $\\alpha=0.02$\n",
        "   * Frozen lake is an episodic task, so here we can simply be interested in the sum of the rewards accumulated during an episode. So we can choose $\\gamma=1$ (no discount)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlwU1ehHT-D5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAB1h59aUCDT"
      },
      "outputs": [],
      "source": [
        "def runningAvg(data, windowSize):\n",
        "  res = np.zeros(len(data)-windowSize)\n",
        "  sum=0\n",
        "  for i in range(windowSize):\n",
        "    sum += data[i]\n",
        "  for i in range(len(data)-windowSize):\n",
        "    res[i]= sum/windowSize\n",
        "    sum -= data[i]\n",
        "    sum += data[i+windowSize]\n",
        "  return res\n",
        "\n",
        "\n",
        "# visualisation de la table Q pour FrozenLake 4x4 et 8x8\n",
        "# passez la taille (4 ou 8) en paramètres\n",
        "def plotQ(q_table, map_size):\n",
        "  if (map_size==4):\n",
        "    MAP = [\n",
        "        \"SFFF\",\n",
        "        \"FHFH\",\n",
        "        \"FFFF\",\n",
        "        \"HFFG\"\n",
        "    ]\n",
        "  else:\n",
        "    MAP=[\n",
        "        \"SFFFFFFF\",\n",
        "        \"FFFFFFFF\",\n",
        "        \"FFFHFFFF\",\n",
        "        \"FFFFFHFF\",\n",
        "        \"FFFHFFFF\",\n",
        "        \"FHHFFFHF\",\n",
        "        \"FHFFHFHF\",\n",
        "        \"FFFHFFFG\"\n",
        "    ]\n",
        "  best_value = np.max(q_table, axis = 1).reshape((map_size,map_size))\n",
        "  best_policy = np.argmax(q_table, axis = 1).reshape((map_size,map_size))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(best_value)\n",
        "\n",
        "  for i in range(best_value.shape[0]):\n",
        "      for j in range(best_value.shape[1]):\n",
        "          if MAP[i][j] in 'GH':\n",
        "              arrow = MAP[i][j]\n",
        "          elif best_policy[i, j] == 0:\n",
        "              arrow = '<'\n",
        "          elif best_policy[i, j] == 1:\n",
        "              arrow = 'v'\n",
        "          elif best_policy[i, j] == 2:\n",
        "              arrow = '>'\n",
        "          elif best_policy[i, j] == 3:\n",
        "              arrow = '^'\n",
        "          if MAP[i][j] in 'S':\n",
        "              arrow = 'S ' + arrow\n",
        "          text = ax.text(j, i, arrow, ha = \"center\", va = \"center\",\n",
        "                         color = \"black\")\n",
        "\n",
        "  cbar = ax.figure.colorbar(im, ax = ax)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHE2QsoFUEvx"
      },
      "source": [
        "## Q-learning\n",
        "Implement the Q-learning algorithm (starting from SARSA, there should only be a few lines of code to modify!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVBGAt8GUP3U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuGBEy-OUQTA"
      },
      "source": [
        "## Comparison\n",
        "\n",
        "Compare the policies found using SARSA, Q-learning, and you should also be able to use the code for the on policy Monte Carlo algorithm from the previous TD.\n",
        "\n",
        "Before convergence to the optimal, we often observe that SARSA has chosen a less risky policy before falling on the optimal for FrozenLake8x8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Or31wpcUV5j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-boRhTquUWki"
      },
      "source": [
        "## Solving a tabular version of Cart-pole\n",
        "\n",
        "Finally, we propose to use your code and to test the learning on the cart-pole problem. A priori, this is a problem where the states are continuous variables. We propose here to discretize the variables and to try to use one of the methods to see your results.\n",
        "\n",
        "The reward you get is the number of time steps where the stick stayed in equilibrium. If you use colab to code, you will unfortunately not be able to view an episode with the render method :-(\n",
        "\n",
        "This Cart-Pole environment involves moving a cart to balance a beam. More precisely:\n",
        "* There are two actions: left and right (represented by 0 and 1).\n",
        "* The received observation (i.e. the state) is a numpy array with 4 variables: the position of the cart, the velocity, the angle to the vertical and the position of the top of the beam.\n",
        "* The episode ends when the angle of the beam to the vertical exceeds 12 degrees.\n",
        "* The rewards received are equal to 1 unless the angle exceeds 12 degrees.\n",
        "\n",
        "Below you are given the functions to perform the discretization and to encode the state into an integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU8r6aJmUnF5",
        "outputId": "653f506f-8a0a-4989-e66f-3fad9bec907f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "environnement with  2  actions\n",
            "the action space is code with a class  Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)  representing a continuous space\n",
            "the lower bounds are:  [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
            "the upper bounds are  [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
            "(array([-0.024397  , -0.21372429,  0.00895479,  0.25025564], dtype=float32), 1.0, False, False, {})\n",
            "Episode terminated after 26 iterations\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "print(\"environnement with \", env.action_space.n, \" actions\")\n",
        "print(\"the action space is code with a class \", env.observation_space,\n",
        "      \" representing a continuous space\")\n",
        "print(\"the lower bounds are: \", env.observation_space.low)\n",
        "print(\"the upper bounds are \",env.observation_space.high)\n",
        "env.reset()\n",
        "nbIt=0\n",
        "done=False\n",
        "print(env.step(np.random.randint(2)))\n",
        "while not done:\n",
        "  nextState, reward, done, info, _ = env.step(np.random.randint(2))\n",
        "  nbIt+=1\n",
        "print(\"Episode terminated after {} iterations\".format(nbIt))\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPVy-tOW6u5"
      },
      "source": [
        "Modify your implementation of Q-learning and/or SARSA to test if you can learn to keep the stick balanced. One modification will be to use the above functions to encode/decode a state. Another modification will probably be to add the number of states as a parameter because this number is now independent of the environment!\n",
        "With $\\epsilon=0.1$, $\\alpha=0.2$ and $\\gamma=0.9$ as parameters, I can reach a score around 90 time steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6iFnX2UUniM",
        "outputId": "f715e798-0344-4dbb-a568-97ed8561cf09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of states is  625\n"
          ]
        }
      ],
      "source": [
        "def discretise(x,mini,maxi):\n",
        "  # discretise x\n",
        "\n",
        "  if x<mini: x=mini\n",
        "  if x>maxi: x=maxi\n",
        "  return int(np.floor((x-mini)*nval/(maxi-mini+0.0001)))\n",
        "\n",
        "#def encode(observation):\n",
        "#  pos = discretise(observation[0],mini=-4.8,maxi=4.8)\n",
        "#  vel = discretise(observation[1],mini=-10,maxi=10)\n",
        "#  angle = discretise(observation[2],mini=-0.42,maxi=0.42)\n",
        "#  pos2 = discretise(observation[3],mini=-1,maxi=1)\n",
        "#  return pos + vel*nval + angle*nval*nval + pos2*nval*nval*nval\n",
        "\n",
        "def encode(observation):\n",
        "  pos = discretise(observation[0],mini=-1,maxi=1)\n",
        "  vel = discretise(observation[1],mini=-1,maxi=1)\n",
        "  angle = discretise(observation[2],mini=-1,maxi=1)\n",
        "  pos2 = discretise(observation[3],mini=-1,maxi=1)\n",
        "  return pos + vel*nval + angle*nval*nval + pos2*nval*nval*nval\n",
        "\n",
        "nval =5 # number of discrete values that a continuous variable can take (granularity)\n",
        "N= nval ** 4 # state space size\n",
        "print(\"The number of states is \", N)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6JNW5bJXslB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
